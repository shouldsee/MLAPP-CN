
## MLAPP 读书笔记 - 15 高斯过程(Gaussian Processes)

> A Chinese Notes of MLAPP，MLAPP 中文笔记项目 
https://zhuanlan.zhihu.com/python-kivy

记笔记的人：[shouldsee](https://www.zhihu.com/people/shouldsee/activities)


### 导论
在监督学习中，我们同时观测输入$x_i$和输出$y_i$，并假设它们符合一个未知的关系f,也即$y_i=f(x_i)$。但这个关系可能是有噪的。 对此，理论上最佳的处理方案是根据观测数据去推断一个在函数空间上的分布$p(f | X,y)$,然后据此推断对应新输入的相应输出，也即计算边缘分布：

$$
p(y_∗ |x_∗ , X, y) = p(y_∗ |f, x_∗ )p(f |X, y)df ~~\text{(15.1)}
$$

本书行进至此，主要考察了关系/函数f的参数化形式，这样一来我们推断的其实是$p(\theta|D)$而不是$p(f |D)$ ($\theta$为$f$的参数)。在本章节中，我们讨论一种在关系$f$本身（而不是其参数$\theta$）上的贝叶斯推断。

这种推断方法是基于高斯过程的（Gaussian Processes），简称GP。高斯过程定义了一个在函数上的先验。这个先验分布结合数据可以给出一个后验分布。 尽管乍一看“函数的分布”是不容易表示的，但实际上我们只需根据这个函数在有限，但任意的点的函数值来定义这个分布，譬如说$x_1,\dots,x_N$。 一个高斯过程假设$p(f(x_1),\dots,f(x_N))$是一个多维高斯分布，具有参数$\mu(x)$和协方差$\Sigma(x)=\Sigma_{ij}=\kappa(x_i,x_j)$，其中$\kappa$是一个正定的和函数(见章节14.2\ref{sec:14.2}核函数的论述)。其要义在于，如果$x_i$和$x_j$在核函数看来是相似的，那么待定函数$f$也应在这两点输出相似的值(见图15.1 \ref{fig:15.1})。 

在回归分析的框架下，以上计算都有闭式解，并且可以在$O(N^3)$的复杂度下计算。 （章节15.6\ref{sec:15.6}会讨论快速近似算法）; 在聚类分析的框架下，我们必须采用一些近似（比如说高斯近似），因为此时的后验分布已经不是纯粹的高斯分布了。 

高斯过程可以看成是章节14所述的核函数操作(包括L1VM,RVM,SVM)的一个贝叶斯式的可选备案。 尽管核函数方法更稀疏也因而更快，它们并不总给出调校好的概率输出(见章节15.4.4的讨论\ref{sec:15.4.4}) 拥有一个良定义(?)的概率输出对有些应用如实时视觉追踪和机械控制(Ko and Fox 2009),强化学习和最优控制(Engel et al. 2005; Deisenroth et al. 2009)，非凸函数的全局优化(Mockus
et al. 1996; Lizotte 2008; Brochu et al. 2009),及实验设计(Santner et al. 2003)等而言是至关重要的。

此处的论述和(Rasmussen and Williams 2006)联系紧密， 细节存疑处可以前往阅读确认。 或也可阅读(Diggle and Ribeiro 2007), 此文同时讨论了和GP紧密相关的，克里金法。后者在在空间统计学中有者广泛的使用。
