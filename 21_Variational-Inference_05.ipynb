{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\newcommand{\\KLDiv}{\\mathbb{KL}}\n",
      "\\newcommand{\\bE}{\\mathbb{E}}\n",
      "\\newcommand{\\bH}{\\mathbb{H}}\n",
      "\\newcommand{\\const}{\\text{const}}\n",
      "$$\n",
      "\\newcommand{\\KLDiv}{\\mathbb{KL}}\n",
      "\\newcommand{\\bE}{\\mathbb{E}}\n",
      "\\newcommand{\\bH}{\\mathbb{H}}\n",
      "\\newcommand{\\const}{\\text{const}}\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "s = open('texAbbr.tex','r').read()\n",
    "print s\n",
    "ipd.display_latex(s)\n",
    "\n",
    "s = '$$%s\\n$$'%s\n",
    "print s\n",
    "ipd.display_markdown(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun 17 Jun 11:57:26 BST 2018\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21.5 变分贝叶斯（变贝）\n",
    "\n",
    "目前为止我们主要专注于在模型参数 $\\theta$ 已知的情况下推断隐变量 $z_i$ 的分布。现在起我们开始考虑推断模型参数其本身。如果我们作一个完全的因子分解（也就是平均场）近似 $p(\\theta | D) \\approx \\prod_k q(\\theta_k)$，我们实际上就在做**变分贝叶斯**(VB:Variational Bayes, @Hinton&Camp1993,@McKay1995a,@Attias2000,@Beal&Ghahramani2006,@Smidl&Quinn2005)了。接下来我们会给出一些在假设不存在隐变量情况下，应用变分贝叶斯的例子，如果我们想要同时推断隐变量和参变量，并作如下近似 $p(\\theta,z_{1:N}|D) \\approx q(\\theta) \\prod_i q_i(z_i)$，我们实际上就在使用变分贝叶斯期望最大化算法(VBEM)，详见Sec21.6\\ref{sec:21.6}。\n",
    "\n",
    "### 21.5.1 实例：单一高斯变量的变贝\n",
    "\n",
    "根据(@MacKay2003,p429)，我们考虑推断一维高斯分布的参数分布 $p(\\mu,\\lambda | D)$,此处$\\lambda=1/\\sigma^2$ 指高斯精度。为简便期间，我们使用一个如下的共轭先验分布\n",
    "\n",
    "$$\n",
    "p(\\mu,\\lambda) = \\mathcal{N}(\\mu|\\mu_0,(\\kappa \\lambda)^{-1})\\text{Ga}(\\lambda|a_0,b_0)\n",
    "~~~~\\text{(21.65)}\\label{eqn:21.65}\n",
    "$$\n",
    "\n",
    "但是呢，我们用来近似后验的是一个如下的因子分布\n",
    "\n",
    "$$\n",
    "q(\\mu,\\lambda) = q_\\mu(\\mu) q_\\lambda(\\lambda)\n",
    "~~~~\\text{(21.66)}\\label{eqn:21.66}\n",
    "$$\n",
    "\n",
    "我们不需要指定分布 $q_\\mu,\\,q_\\lambda$ 的确切形式，因为其最优形式会在推导的时候自动“出现”(方便的是，它们恰巧是相应的高斯分布和伽马分布)\n",
    "\n",
    "由于我们在Sec4.6.3.7\\ref{sec:4.6.3.7}已经得出了计算该模型确切后验的方法，因此如上处理的动机并不是很显然。动机主要有如下两条：首先这是一个有教学意义的练习，我们可以乘机通过与确切分布的对比对比，来分析近似分布的质量；其次，这个方法可以很自然地加以改造以适用一个半共轭的先验 $p(\\mu,\\lambda)=\\mathcal{N}(\\mu|\\mu_0,\\tau_0)\\text{Ga}(\\lambda|a_0,b_0)$，而这种先验下是无法进行确切推断的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21.5.1.1 目标分布\n",
    "\n",
    "未正规化的对数后验具有形式\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log \\tilde{p}(\\mu,\\lambda) = \\log p(\\mu,\\lambda,D) \n",
    "&= \\log p(D|\\mu,\\lambda) + \\log p(\\mu|\\lambda) + \\log p(\\lambda)\n",
    "~~~~\\text{(21.67)}\\label{eqn:21.67}\n",
    "\\\\\n",
    "&=\\frac{N}{2}\\log\\lambda - {\\lambda \\over 2 } \\sum_{i=1}^N (x_i - \\mu)^2 - \\frac{\\kappa_0 \\lambda}{2}(\\mu-\\mu_0)^2 \n",
    "\\\\ & ~~ +{1\\over 2}\\log(\\kappa_0 \\lambda) + (a_0 - 1)\\log \\lambda - b_0 \\lambda + \\text{const}  \n",
    "~~~~\\text{(21.68)}\\label{eqn:21.68}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### 21.5.1.2 更新 $q_\\mu（\\mu）$\n",
    "\n",
    "通过对 $\\lambda$ 求期望可以得到 $q_\\mu(\\mu)$ 的最优形式：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log q_\\mu(\\mu) &= \\bE_{q_\\lambda}[\\log p(D|\\mu,\\lambda) + \\log p(\\mu|\\lambda) ] + \\text{const} \n",
    "~~~~\\text{(21.69)}\\label{eqn:21.69}\n",
    "\\\\\n",
    "&= -\\frac{\\bE[\\lambda]}{2} \\left\\{\n",
    "\\kappa_0 (\\mu - \\mu_0)^2 + \\sum_{i=1}^N (x_i - \\mu)^2\n",
    "\\right \\} + \\text{const}\n",
    "~~~~\\text{(21.70)}\\label{eqn:21.70}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "通过展开平方项（？疑）可以证明 $q_\\mu(\\mu) = \\mathcal(\\mu | \\mu_N , \\kappa^{-1}_N)$,且有\n",
    "\n",
    "$$\n",
    "\\mu_N={ \\kappa_0 \\mu_0 + N \\bar{x} \\over \\kappa_0 + N }, \\kappa_N = (\\kappa_0 + N ) \\bE_{q_\\lambda}[\\lambda]\n",
    "~~~~\\text{(21.71)}\\label{eqn:21.71}\n",
    "$$\n",
    "\n",
    "此时我们还不知道 $q_\\lambda(\\lambda)$ 的确切分布所以无法计算 $\\bE[\\lambda]$，但我们马上就会解决这个玩意\n",
    "\n",
    "#### 21.5.1.3 更新 $q_\\lambda (\\lambda)$\n",
    "\n",
    "$q_\\lambda(\\lambda)$ 的最优分布有如下表示\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log q_\\lambda(\\lambda) \n",
    "&= \\bE_{q_\\mu}[ \n",
    "\\log p(D|\\mu,\\lambda) + \\log p(\\mu | \\lambda ) + \\log p(\\lambda)  ] + \\const\n",
    "~~~~\\text{(21.72)}\\label{eqn:21.72}\n",
    "\\\\\n",
    "&= (a_0 - 1 )\\log \\lambda -　b_0 \\lambda + {1\\over2}\\log\\lambda + {N\\over2} \\log\\lambda \n",
    "\\\\\n",
    "&~~-{\\lambda\\over2}\\bE_{q_\\mu}\\left[\n",
    "\\kappa_0 (\\mu - \\mu_0)^2 +  \\sum_{i=1}^N (x_i-\\mu)^2\n",
    "\\right] + \\const\n",
    "~~~~\\text{(21.73)}\\label{eqn:21.73}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "可以发现这对应一个伽马分布，因而有　$q_\\lambda(\\lambda)=Ga(\\lambda|a_N,b_N)$,且有\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_N &= a_0 + \\frac{N+1}{2}\n",
    "~~~~\\text{(21.74)}\\label{eqn:21.74}\n",
    "\\\\\n",
    "b_N &= b_0 + \\frac{1}{2}\\bE_{q_\\mu}\\left[\n",
    "\\kappa_0 (\\mu - \\mu_0)^2 +  \\sum_{i=1}^N (x_i-\\mu)^2\n",
    "\\right]\n",
    "~~~~\\text{(21.75)}\\label{eqn:21.75}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun 17 Jun 13:40:46 BST 2018\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21.5.1.4 计算期望\n",
    "\n",
    "为了实行上述更新，我们必须确定如何计算诸多期望值。鉴于 $q(\\mu) = \\mathcal(\\mu | \\mu_N,\\kappa_N^{-1})$,我们得出\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bE_{q_(\\mu)}[\\mu] &= \\mu_N\n",
    "~~~~\\text{(21.76)}\\label{eqn:21.76}\n",
    "\\\\\n",
    "\\bE_{q(\\mu)}[\\mu^2] &= {1\\over \\kappa_N} + \\mu_N^2\n",
    "~~~~\\text{(21.77)}\\label{eqn:21.77}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "鉴于 $q(\\lambda) = \\text{Ga}(\\lambda \\gvn a_N, b_N)$，我们有\n",
    "\n",
    "$$\n",
    "\\bE_{q(\\lambda)}[\\lambda] = \\frac{a_N}{b_N}\n",
    "~~~~\\text{(21.78)}\\label{eqn:21.78}\n",
    "$$\n",
    "\n",
    "如此我们可以显式地给出更新方程。对 $q(\\mu)$ 有\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_N \n",
    "&= \\frac{\\kappa_0 \\mu_0 + N \\bar{x}}{\\kappa_0 + N}\n",
    "~~~~\\text{(21.79)}\\label{eqn:21.79}\n",
    "\\\\\n",
    "\\kappa_N \n",
    "&= (\\kappa_0 + N ) \\frac{a_N}{b_N}\n",
    "~~~~\\text{(21.80)}\\label{eqn:21.80}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "相应地对 $q(\\lambda)$ 有\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_N &= a_0 + \\frac{N+1}{2}\n",
    "~~~~\\text{(21.81)}\\label{eqn:21.81}\n",
    "\\\\\n",
    "b_N &= b_0 + \\kappa_0 (\\bE[\\mu^2] +\\mu_0^2 -2\\bE[\\mu]\\mu_0 ) \n",
    "+ \\frac{1}{2}\\sum_{i=1}^N (x_i^2 + \\bE[\\mu^2] -2 \\bE[\\mu ] x_i)\n",
    "~~~~\\text{(21.82)}\\label{eqn:21.82}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "可以看到 $\\mu_N$ 和 $a_N$ 实际上是固定的常数，且只有 $\\kappa_N\\,,b_N$需要被迭代更新。(事实上，我们可以利用迭代方程，解析地求出$\\kappa_N\\,\\,b_N$的不动点，但是在此不作展示，只介绍迭代更新法)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21.5.1.5 展示\n",
    "\n",
    "配图Fig21.5\\ref{fig:21.5}给出了一个工作示意。绿色的轮廓表示确切的高斯-伽马后验分布，虚线的轮廓表示的是变分近似在不同的迭代步的结果。可以看到最终的近似是和确切解较为相似的。然而，近似解比真实分布更为“紧致”。事实上平均场近似推断常常欠估计后验的不确定性，更多有关讨论请见Sec21.2.2\\ref{sec:21.2.2}。\n",
    "\n",
    "#### 21.5.1.6 下界\n",
    "\n",
    "在变贝中，我们最大化的是 $L(q)$ 是数据的边际似然值的下界。\n",
    "\n",
    "$$\n",
    "L(q) \\le \\log p(D) = \\log \\int \\int p(D\\mu) p(\\mu, \\lambda) d\\mu d\\lambda\n",
    "~~~~\\text{(21.83)}\\label{eqn:21.83}\n",
    "$$\n",
    "\n",
    "出于多种原因，计算这个下界本身是很有用的。首先，下界可以用来检查算法是否收敛了；其次，它可以用来检查算法的正确性：和期望最大算法(EM)的情况一样，如果下界不是单调增加的话，那一定是哪里出幺蛾子了；再次，下界可以用作一个对数据边际似然的近似，因而可以用来做贝叶斯模型选择。\n",
    "\n",
    "不幸的是，计算这个下界需要经过一些很是心累的代数处理。以下我们会给出这个特例的下界的细节，但我们将不经证明，甚至不加讨论地直接给出下界的结果，因为这样子会简洁许多（?我的天）。\n",
    "\n",
    "对这个模型，我们可以如下计算似然函数 $L(q)$ \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(q) \n",
    "&= \\int \\int q(\\mu, \\lambda) \\log \n",
    "\\frac{ p(D,\\mu,\\lambda) }\n",
    "{q(\\mu,\\lambda) }\n",
    "d\\mu d\\lambda\n",
    "~~~~\\text{(21.84)}\\label{eqn:21.84}\n",
    "\\\\\n",
    "&= \\bE [\\log p(D\\ gvn \\mu,\\lambda) ] \n",
    "+ \\bE[ \\log p(\\mu \\gvn \\lambda) ] \n",
    "+ \\bE[ \\log p(\\lambda) ] \n",
    "\\\\ \n",
    "& ~~ - \\bE[\\log q(u)] - \\bE[ \\log q(\\lambda) ]  \n",
    "~~~~\\text{(21.85)}\\label{eqn:21.85}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "以上所有的期望都是关于 $q(\\mu,\\lambda)$。可以看出最后两项不过是高斯和伽马分布的熵，因此可以直接写出\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bH ( \\mathcal{N}(\\mu_N,\\kappa_N^{-1} ) &= \n",
    "-\\frac{1}{2} \\log \\kappa_N + \\frac{1}{2} (1 + \\log (2\\pi))\n",
    "~~~~\\text{(21.86)}\\label{eqn:21.86}\n",
    "\\\\\n",
    "\\bH(\\text{Ga(a_N,b_N)}) \n",
    "&= \\log \\Gamma(a_N) - (a_N - 1) \\psi (a_N) - \\log (b_N) + a_N\n",
    "~~~~\\text{(21.87)}\\label{eqn:21.87}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "此处 $\\psi()$ 是双伽马函数。\n",
    "\n",
    "为了计算其它的项目，我们要用到如下结果\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bE [ \\log x \\gvn x \\sim \\text{Ga}(a,b)] \n",
    "&= \\psi(a) - \\log(b) \n",
    "~~~~\\text{(21.88)}\\label{eqn:21.88}\n",
    "\\\\\n",
    "\\bE [ x \\gvn x \\sim \\text{Ga}(a,b) ] \n",
    "&= {a \\over b}\n",
    "~~~~\\text{(21.89)}\\label{eqn:21.89}\n",
    "\\\\\n",
    "\\bE [ x \\gvn x \\sim \\mathcal{N}(\\mu, \\sigma^2) ]\n",
    "&= \\mu\n",
    "~~~~\\text{(21.90)}\\label{eqn:21.90}\n",
    "\\\\\n",
    "\\bE [ x^2 \\gvn x \\sim \\mathcal{N} (\\mu, \\sigma^2)] \n",
    "&= \\mu^2 + \\sigma^2 \n",
    "~~~~\\text{(21.91)}\\label{eqn:21.91}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "可以证明对数似然的期望符合下式\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bE_{q(\\mu,\\lambda)}&[ \\log p(D \\gvn \\mu \\lambda) ] \n",
    "~~~~\\text{(21.92)}\\label{eqn:21.92}\n",
    "\\\\\n",
    "&= -{N\\over 2 }\\log(2\\pi) + {N\\over 2} \\bE_{q_\\lambda}[\\log \\lambda] - \\frac{\\bE_{q_\\lambda}[ \\lambda]}{2} \\sum_{i=1}^N \n",
    "\\bE_{q(\\mu)} [ (x_i - \\mu)^2 ] \n",
    "~~~~\\text{(21.93)}\\label{eqn:21.93}\n",
    "\\\\\n",
    "&= -{N\\over 2 }\\log(2\\pi) + {N\\over 2} (\\psi(a_N) - \\log b_N)\n",
    "\\\\\n",
    "& ~~ - {N a_N \\over 2 b_N} \\left( \\hat{\\sigma}^2 + \\bar{x}^2 - \n",
    "2\\mu_N \\bar{x} + \\mu^2_N + \\frac{1}{\\kappa_N}\n",
    "\\right)\n",
    "~~~~\\text{(21.94)}\\label{eqn:21.94}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "此处的 $\\bar{x},\\,\\hat{\\sigma}^2$ 对应观测平均和方差。\n",
    "\n",
    "对$\\lambda$ 的对数先验的期望可做如下处理\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bE_{q(\\lambda)}[ \\log p(\\lambda) ] \n",
    "~~&=~~ (a_0 - 1)\\bE[ \\log \\lambda] -b_0 \\bE [\\lambda ] + a_0\\log b_0 -\\log \\Gamma ( a_0 )\n",
    "~~~~\\text{(21.95)}\\label{eqn:21.95}\n",
    "\\\\\n",
    "~~&=~~ (a_0 - 1)(\\psi(a_N) - \\log b_N) -b_0 \\frac{a_N}{b_N}\n",
    "+ a_0\\log b_0 -\\log \\Gamma ( a_0 )\n",
    "~~~~\\text{(21.96)}\\label{eqn:21.96}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "对$\\mu$ 的对数先验的期望可做如下处理\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bE_{q(\\mu, \\lambda)}[ \\log p(\\mu \\gvn \\lambda) ] \n",
    "~~&=~~ \\frac{1}{2} \\log \\frac{\\kappa_0}{2\\pi} + \\frac{1}{2} \n",
    "\\bE[ \\log \\lambda]q(\\lambda) - \\frac{1}{2} \\bE_{q(\\mu,\\lambda)}\n",
    "[ (\\mu - \\mu_0)^2 \\kappa_0 \\lambda ]\n",
    "\\\\\n",
    "~~&=~~ \\frac{1}{2} \\log \\frac{\\kappa_0}{2\\pi} + \n",
    "\\frac{1}{2}  (\\psi(a_N) - \\log b_N) \n",
    "\\\\\n",
    "& ~~ - \\frac{\\kappa_0}{2} \\frac{a_N}{b_N} \n",
    "\\left[ (\\mu_N - \\mu_0)^2 + \\frac{1}{\\kappa_N}  \\right]\n",
    "~~~~\\text{(21.97)}\\label{eqn:21.97}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "将以上结果结合，可以得到对数似然的下界\n",
    "\n",
    "$$\n",
    "L(q) = {1 \\over 2 }\\log {1 \\over \\kappa_N} + \\log \\Gamma(a_N) - a_N \\log b_N + \\const\n",
    "~~~~\\text{(21.98)}\\label{eqn:21.98}\n",
    "$$\n",
    "\n",
    "这个量随着变贝的迭代更新是单调递增的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun 17 Jun 15:08:31 BST 2018\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.5.2 实例： 线性回归中的变贝\n",
    "\n",
    "***(？略略略）***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 21_Variational-Inference_01-02.ipynb to markdown\n",
      "[NbConvertApp] Writing 6542 bytes to 21_Variational-Inference_01-02.md\n",
      "[NbConvertApp] Converting notebook 21_Variational-Inference_05.ipynb to markdown\n",
      "[NbConvertApp] Writing 7713 bytes to 21_Variational-Inference_05.md\n",
      "[NbConvertApp] Converting notebook 21_Variational-Inference_06.ipynb to markdown\n",
      "[NbConvertApp] Writing 9441 bytes to 21_Variational-Inference_06.md\n",
      "[NbConvertApp] Converting notebook 21_Variational-Inference_01-02.ipynb to html\n",
      "[NbConvertApp] Writing 259214 bytes to 21_Variational-Inference_01-02.html\n",
      "[NbConvertApp] Converting notebook 21_Variational-Inference_05.ipynb to html\n",
      "[NbConvertApp] Writing 260156 bytes to 21_Variational-Inference_05.html\n",
      "[NbConvertApp] Converting notebook 21_Variational-Inference_06.ipynb to html\n",
      "[NbConvertApp] Writing 263444 bytes to 21_Variational-Inference_06.html\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ALI=\"21_Variational-Inference\"\n",
    "for FMT in markdown html\n",
    "do\n",
    "    jupyter nbconvert --TemplateExporter.exclude_code_cell=True ${ALI}*.ipynb --to $FMT\n",
    "done\n",
    "cat ${ALI}_*.md > .${ALI}.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12M\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  65K Jun 16 21:24 00 PreFace.pdf\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 2.6K Jun 16 21:24 00 PreFace.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  35K Jun 16 21:24 01 Introduction.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 698K Jun 16 21:24 01 Introduction.pdf\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  59K Jun 16 21:24 02 Probability.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  48K Jun 16 21:24 03 Generative models for discrete data.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 3.2M Jun 16 21:24 02 Probability.pdf\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  84K Jun 16 21:24 04 Gaussian models.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 2.2M Jun 16 21:24 03 Generative models for discrete data.pdf\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  80K Jun 16 21:24 05 Bayesian statistics.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 532K Jun 16 21:24 04 Gaussian models.pdf\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 3.6M Jun 16 21:24 05 Bayesian statistics.pdf\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 2.0K Jun 16 21:24 Abbreviations.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 2.9K Jun 16 21:24 15_Gaussian-Processes.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  14K Jun 16 21:24 06 Frequentist statistics.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 5.2K Jun 16 21:24 README.md\r\n",
      "-rwxrwxr-x 1 shouldsee shouldsee 249K Jun 16 21:24 MLAPP.png\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  35K Jun 16 21:24 LICENSE\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  627 Jun 16 21:24 issue_template.md\r\n",
      "drwxrwxr-x 2 shouldsee shouldsee 4.0K Jun 16 21:24 Figure\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 4.8K Jun 17 10:10 15_Gaussian-Processes_01-intro.ipynb\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  79K Jun 17 16:10 21_Variational-Inference_01-02.pdf\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  73K Jun 17 16:10 21_Variational-Inference_01-02.ipynb\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  187 Jun 17 18:26 texAbbr.tex\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  985 Jun 17 18:26 texAbbr.ipynb\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  22K Jun 17 22:29 21_Variational-Inference_06.ipynb\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  11K Jun 17 22:30 21_Variational-Inference_01-02.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  11K Jun 17 22:30 21_Variational-Inference_05.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  15K Jun 17 22:30 21_Variational-Inference_06.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 258K Jun 17 22:30 21_Variational-Inference_01-02.html\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 257K Jun 17 22:30 21_Variational-Inference_05.html\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee 262K Jun 17 22:30 21_Variational-Inference_06.html\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  35K Jun 17 22:30 21_Variational-Inference.md\r\n",
      "-rw-rw-r-- 1 shouldsee shouldsee  18K Jun 17 22:30 21_Variational-Inference_05.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lhtr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
